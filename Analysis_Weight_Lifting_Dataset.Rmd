---
title: "Analysis of the Weight Lifting Exercise Dataset"
author: "V. Taralova"
date: "December 18, 2016"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

# Introduction 

In this project we analyse a data from acceleration and position sensors put on the belt, forearm, arm, and dumbell of 6 participants.
The data set can be downloaded from https://archive.ics.uci.edu/ml/datasets/Weight+Lifting+Exercises+monitored+with+Inertial+Measurement+Units. More information is available at: http://groupware.les.inf.puc-rio.br/har#ixzz4TBbNIRh9, as well as in the paper: "Qualitative Activity Recognition of Weight Lifting Exercises", Velloso, E., Bulling, A., Gellersen, H., Ugulino, W. and Fuks, H., Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13), Stuttgart, Germany: ACM SIGCHI, 2013. 

The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The correctly performed exercises are labeled as Class A in the data set and the various incorrectly performed exercises are labeled as classes B to E depending on the mistakes made. 

Our goal is, based on the given measurements, to build a model that can predict whether an exercise is performed correctly or not for new data.   


# Reading the Data

```{r}

setwd("D:/Coursera_Practical_Machine_Learning/Final_Project")

db.train = read.table("pml-training.csv", sep = ",", header = TRUE)

db.test = read.table("pml-testing.csv", sep = ",", header = TRUE)

```

The data frame "db.train" has `r dim(db.train)[1]` observations of `r dim(db.train)[2]` variables:

```{r}

dim(db.train)

```

# Cleaning Missing Values

First we check for the percentage of missing values in each variable. For this purpose we create a list containing the columns of the data frame "db.train" as its elements: 

```{r}

db.columns.list <- as.list(db.train)

```

Then we use the R function "sapply()" from the base installation to create the numeric vectors "sum.NA" and "percentage.NA" which contain the number and percentage of NA values for each of the `r dim(db.train)[2]` variables: 

```{r}

sum.NA <- sapply(db.columns.list, function(v){
  
  sum(is.na(v))
  
}, simplify = TRUE)

percentage.NA <- sapply(db.columns.list, function(v){
  
  mean(is.na(v))
  
}, simplify = TRUE)

```

It turns out that the number of variables with missing values is `r length(which(percentage.NA > 0))`:

```{r}

length(which(sum.NA > 0))

```

Next, we extract the indices of the variables with missing values and we display the names of these variables along with the respective percentage of NA values:

```{r}

indices.NA <- which(sum.NA > 0)

percentage.NA[indices.NA] 

```

All of the variables with non-zero number of NA values have more than 90% missing values:

```{r}

length(which(percentage.NA[indices.NA] > 0.9))==length(which(sum.NA > 0)) 
```

Looking at the data frame we notice that there are also many variables containing nonsense values, such as "" (empty fields) and "#DIV/0!", e.g.:

```{r}

head(db.train[, 70:75], 10)

```

We investigate these variables and we remove them. We create the vectors "number.nomeaning" and "percentage.NM" containing respectively the number and percentage of "" or "#DIV/0!" values for each variable in the training data set:

```{r}

number.nomeaning <- sapply(db.columns.list, function(v){
  
  sum(v == "" | v == "#DIV/0!")
  
}, simplify = TRUE)


percentage.NM <- sapply(db.columns.list, function(v){
  
  mean(v == "" | v == "#DIV/0!")
  
}, simplify = TRUE)

```

The number of variables with "" or "#DIV/0!" values is:

```{r}

length(which(number.nomeaning > 0))

```

Below we show the variables containing non-meaningful values together with the respective percentage of these values:

```{r}

indices.NM <- which(number.nomeaning > 0)

percentage.NM[indices.NM] 

```

All of the variables with non-zero number of non-meaningful values have more than 90% missing values:

```{r}

length(which(percentage.NM[indices.NM] > 0.9))==length(which(number.nomeaning > 0))

```

We check if there is an overlap between the variables containig missing values (NA) and the variables containing non-meaningful values:

```{r}

intersect(indices.NM, indices.NA)

```

It turns out there is no overlap between the two sets of variables.

Now we remove from the data frame all variables containing missing or non-meaningful values:

```{r}

both.indices <- c(indices.NA, indices.NM)

db.train <- db.train[, -both.indices]

db.test <- db.test[, -both.indices]

```

We removed a total of `r length(both.indices)` variables:

```{r}

length(both.indices)

```

Further, we exclude also the variables "X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window" and "num_window" (the first seven columns from the data frame), because we do not need them to predict  whether or not an exercise is done correctly:

```{r}

ind <- c(1:7) # the indices of the columns we want to remove

db.train = db.train[,-ind]

db.test = db.test[,-ind]

```

# Fitting a Model

## Partition of the "db.train" data set into a new training data set and a validation data set

First we partition the training data set into new training data set and a cross validation data set. We use 70% of the training data set for training and the other 30% for cross validation:

```{r, message = FALSE}

library(caret)

set.seed(35721)

inTrain = createDataPartition(y = db.train$classe, p = 0.7, list = FALSE)

db.training = db.train[inTrain, ]

db.validation = db.train[-inTrain, ]

```

The dimensions of the new data sets are given below:

```{r}

dim(db.training)

dim(db.validation)

```


## Support Vector Machines (SVM)

We fit a model with the support vector machines (svm) algorithm, which is implemented in the R package "e1071". This is a popular method that usually has reasonable default performance and has parameters that allow for further tuning if necessary. It also allows for fitting both linearly and non-linearly separable data using linear or non-linear kernels, respectively.  
By default, the "svm()" function in the "e1071" package scales the data. 

First we train an SVM model with kernel set to "linear": 

```{r SVMlinear, message = FALSE}

library(e1071)

set.seed(45112)

start_time <- proc.time()
ModSVM.lin <- svm(classe~., data = db.training, kernel = "linear")
proc.time() - start_time

```

The computational time is approximately 45s. The vector "predSVM.lin" contains the predictions on the training data set and the vector "predSVM.valid.lin" contains the predictions on the validation data set. The accuracy on the training data is 78.99% and the "out of sample" accuracy is 78.79%. As expected, the "out of sample" accuracy is less than the "in sample" accuracy (and respectively, the "out of sample" error is greater than the "in sample" error). We also display the respective confusion matrices. 

```{r}

predSVM.lin <- predict(ModSVM.lin, newdata = db.training)

mean(predSVM.lin == db.training$classe)

confusionMatrix(data = predSVM.lin, reference = db.training$classe)

```


```{r}

predSVM.valid.lin <- predict(ModSVM.lin, newdata = db.validation)

mean(predSVM.valid.lin == db.validation$classe)

confusionMatrix(data = predSVM.valid.lin, 
                reference = db.validation$classe)


```

The above results suggests that the data is not linearly separable. Therefore, we try to fit an SVM model with the default radial kernel:

```{r SVMradial}

set.seed(45112)

start_time <- proc.time()
ModSVM <- svm(classe~., data = db.training)
proc.time() - start_time

```

In this case the computational time is approximately 48s. The vector "predSVM" contains the predictions on the training data set and the vector "predSVM.valid" contains the predictions on the validation data set. The "in sample" accuracy is 95.95% and the "out of sample" accuracy is 95.23%. Below we also display the respective confusion matrices:

```{r}

predSVM <- predict(ModSVM, newdata = db.training)

mean(predSVM == db.training$classe)

confusionMatrix(data = predSVM, reference = db.training$classe)

```


```{r}

predSVM.valid <- predict(ModSVM, newdata = db.validation)

mean(predSVM.valid == db.validation$classe)

confusionMatrix(data = predSVM.valid, reference = db.validation$classe)


```

The radial kernel already gives us very high accuracy, however, as per the suggestions in the documentation of the "e1071" package, we also perform a grid search to find the optimal parameters C and $\gamma$. 
We follow the recommendations from the paper "A Practical Guide to Support Vector Classification" by Hsu, Chang, and Lin (<http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf>) and use exponentially growing C and $\gamma$. We also do not use cross validation but a fixed split for the sampling in the "tune.control":

```{r tuneSVM}

set.seed(45112)

start_time <- proc.time()
Model.TunedSVM <- tune(svm, classe~., data = db.training,
                       ranges = list(gamma = 2^seq(-9,-3,2),
                                     cost = 2^seq(4,10,2)), 
                       tunecontrol = tune.control(sampling = "fix"))
proc.time() - start_time

```

We now plot the achieved error on the grid and show the summary of the model:  

```{r}

plot(Model.TunedSVM)

summary(Model.TunedSVM)

```

The lowest error is achieved at $\gamma=0.3$ and C=256. We now extract the best model which uses these parameters and check its accuracy:

```{r}

BestTunedSVM <- Model.TunedSVM$best.model

mean(predict(BestTunedSVM, newdata = db.training) == db.training$classe)
mean(predict(BestTunedSVM, newdata = db.validation) == db.validation$classe)

```

The accuracy is above 99.5% on both the training set and the validation set. Hence, our parameter tuning proved successful and we managed to improve the initial accuracy. However, the computational time increased to almost 400s since we had to train 16 models. 
 
## Random Forest

Here we fit a model with the random forest algorithm with the "randomForest()" function from the "randomForest" package. We use the default parameters, like the number of variables for each split (which in this case is $\sqrt{52}\approx 7$) and the number of trees, which is 500.

```{r, message = FALSE}

library(randomForest)

set.seed(1234)

start_time <- proc.time()
ModRF <- randomForest(classe~., data = db.training)
proc.time() - start_time

```

The estimated "out of sample" error is 0.44%:

```{r}

plot(ModRF)

print(ModRF)

```

Below we test the accuracy on the training and the validation sets and we see that it is 100% and 99.52%, respectively, which is in agreement with the OOB estimate. 

```{r}

mean(predict(ModRF, newdata = db.training) == db.training$classe)

mean(predict(ModRF, newdata = db.validation) ==
       db.validation$classe)

```

From the error plot of the random forest model, we see that the error does not decrease after the number of trees reaches a certain threshold. Hence, we try to build a RF model with 80 trees to check if we can achieve the same accuracy with a computationally less expensive model:

```{r}

set.seed(1234)

start_time <- proc.time()
ModRF.small <- randomForest(classe~., data = db.training, ntree = 100)
proc.time() - start_time

```

On our machine the computational time went from about 36s to 7s, which is 5 times faster. From the code below, we see that the OOB error estimate increased a little bit to 0.55% and the accuracy is again 100% on the training data and decreased by less than 0.2% on the validation set.

```{r}

print(ModRF.small)

mean(predict(ModRF.small, newdata = db.training) == db.training$classe)

mean(predict(ModRF.small, newdata = db.validation) ==
       db.validation$classe)

```

### Notes

We also tried the "train()" function from the "caret" package. However, for this data set it was much more computationally expensive with its default settings and it did not give better results. Therefore, we provide only results for "svm()" and "randomForest()". The given computational times are on a laptop with Intel i7 5500U CPU and 8GB DDR3-1600 RAM.

# Predicting the outcomes of the test cases

We use the two fitted models to predict the outcomes for the test data set. 

```{r}

predict(BestTunedSVM, newdata = db.test) 

predict(ModRF, newdata = db.test) 

sum(predict(BestTunedSVM, newdata = db.test) != predict(ModRF, newdata = db.test))

```

We see that the two models give identical results.
